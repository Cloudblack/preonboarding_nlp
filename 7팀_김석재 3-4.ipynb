{"cells":[{"cell_type":"markdown","metadata":{"id":"592U6lXs3d2t"},"source":["# Week3_4 Assignment\n","\n","## [BASIC](#Basic) \n","- Encoder & Decoder Layer 코드를 직접 필사하고 각 함수에 주석을 달 수 있다. \n","\n","## [CHALLENGE](#Challenge)\n","- 텐서의 크기(shape)를 계산할 수 있다. \n","\n","## [ADVANCED](#Advanced)\n","- 완성된 transformer 모델의 모든 학습 가능한 파라미터 이름과 크기(shape)를 출력할 수 있다.\n","\n","### Informs\n","이번 과제에서는 \"[Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)\"의 코드를 필사해본다.   \n","\"Annotated Transformer\"는 \"Attention is all you need\" 논문에서 제안한 transformer 모델을 pytorch 라이브러리로 직접 구현한다.   \n","코드 필사를 통해 다음을 배울 수 있다.    \n","- Encoder, Decoder 구조\n","- Attention Mechanism\n","- \"residual connection\", \"layer normalization\" 등의 구조 "]},{"cell_type":"markdown","metadata":{"id":"GoebvnNZ99r-"},"source":["코드 필사를 시작하기 앞서, transformer 모델의 최종 구조를 살펴보자.    \n","\n","<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_full.png?raw=true\" width=\"500\" align=\"center\"/>\n","\n","최종 모델은 `EncoderDecoder()` 클래스에 여러 인스턴스를 생성자의 입력 파라미터로 넣어 생성한다.    \n","앞으로 우리는 `EncoderDecoder()` 클래스와 같은 여러 클래스들을 구현하고 연결할 것이다. 따라서 대략적인 클래스간의 관계를 살펴보고 이해한다면 보다 큰 그림을 가지고 코드 필사를 할 수 있을 것이다. "]},{"cell_type":"markdown","metadata":{"id":"DB6cNaXP99sB"},"source":["Transformer 모델은 크게 4가지 클래스로 구현된다.    \n","- Frame\n","    - frame 역할을 하는 `EncoderDecoder` 클래스\n","- Input Embedding & Encoding\n","    - 입력값을 벡터화하는 `Embeddings`, `PositionalEncoding`\n","- Encoder & Decoder\n","    - 각 6개 layer를 갖고 있는 `Encoder`, `Decoder`\n","    - layer 1층을 구현한 `EncoderLayer`, `DecoderLayer`\n","- Sublayer\n","    - `EncoderLayer`, `DecoderLayer` 내부에서 사용되는 Sublayer 클래스인 `MultiHeadAttiontion`, `PositionwiseFeedForward`\n","    - Sublayer 클래스들을 연결하는 `SublayerConnection`\n","    \n","아래 좌측 도식에서 각 클래스의 색상은 아래 우측 도식(transformer 구조)의 색상과 맵핑되어 있다.    \n","각 클래스의 역할과 클래스 간 연결 관계를 생각하면서 transformer를 코드로 구현해보자.   \n","\n","\n","<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_map.png?raw=true\" width=\"400\" height=\"400\" align=\"left\"/>\n","<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_transformer.png?raw=true\" width=\"300\" height=\"400\" align=\"right\"/>\n","\n"]},{"cell_type":"code","execution_count":95,"metadata":{"id":"qaadVYo799sE","executionInfo":{"status":"ok","timestamp":1646901660655,"user_tz":-540,"elapsed":473,"user":{"displayName":"김석재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinQOomWONmt82tKAM95-_bF9gvg7EPjUXHZ02Hu4s=s64","userId":"05383809141048124324"}}},"outputs":[],"source":["import os\n","import sys\n","import pandas as pd\n","import numpy as np \n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","import math, copy, time\n","import random"]},{"cell_type":"markdown","metadata":{"id":"1OEO0al299sJ"},"source":["## Basic"]},{"cell_type":"markdown","metadata":{"id":"OKKyKfqB99sL"},"source":["### Frame\n","- `EncoderDecoder`\n","\n","아래 도식은 `EncoderDecoder` 클래스의 `forward()`, `encode()`, `decode()` 메소드를 도식화 한 것이다.    \n"," \n","<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_encoderdecoder.png?raw=true\" width=500>\n","\n","\n","- `Generator`"]},{"cell_type":"code","execution_count":96,"metadata":{"id":"MECCTGpt99sP","executionInfo":{"status":"ok","timestamp":1646901661283,"user_tz":-540,"elapsed":13,"user":{"displayName":"김석재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinQOomWONmt82tKAM95-_bF9gvg7EPjUXHZ02Hu4s=s64","userId":"05383809141048124324"}}},"outputs":[],"source":["class EncoderDecoder(nn.Module): \n","    \"\"\"\n","    표준 인코더 아키텍쳐, 많은 모델의 기본\n","    \"\"\"\n","    \n","    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n","        super(EncoderDecoder, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.src_embed = src_embed #src = 소스\n","        self.tgt_embed = tgt_embed #tgt = 타겟\n","        self.generator = generator \n"," \n","    \n","    \n","    def forward(self, src, tgt, src_mask, tgt_mask):\n","         return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask) # intput (src,src_mask)=>src_embed,src_mask => enc => enc output(memory), src_mask, intput(tgt, tgt_mask) => tgt_embed=> decode\n","      \n","    \n","    \n","    def encode(self, src, src_mask):\n","        return self.encoder(self.src_embed(src),src_mask) #src =>src_embed,src_mask => enc\n","    \n","    \n","    def decode(self, memory, src_mask, tgt, tgt_mask): #enc output(memory) ,tgt => tgt_embed => decode\n","        return self.decoder(self.tgt_embed(tgt),memory, src_mask, tgt_mask)"]},{"cell_type":"code","execution_count":97,"metadata":{"id":"Py2wcYPX99sT","executionInfo":{"status":"ok","timestamp":1646901661284,"user_tz":-540,"elapsed":13,"user":{"displayName":"김석재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinQOomWONmt82tKAM95-_bF9gvg7EPjUXHZ02Hu4s=s64","userId":"05383809141048124324"}}},"outputs":[],"source":["class Generator(nn.Module):\n","    \"Define standard linear + softmax generation step. 레이어 마지막에 linear와 softmax 부분\"\n","    \n","    def __init__(self, d_model, vocab):\n","        super(Generator,self).__init__()\n","        self.proj = nn.Linear(d_model,vocab) \n","\n","    \n","    \n","    def forward(self, x):\n","        return F.log_sfotmax(self.proj(x), dim=-1) #모델에서 사용하는 부분을 찾지 못했다 어디서 사용되는건지..\n"]},{"cell_type":"markdown","metadata":{"id":"xI-5SRHD99sX"},"source":["### Encoder\n","- `Encoder`\n","- `EncoderLayer`\n","- `SublayerConnection`\n","- Reference\n","    - Layer Normalization\n","        - [한국어 설명](https://yonghyuc.wordpress.com/2020/03/04/batch-norm-vs-layer-norm/)\n","        - [torch official docs](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)\n","    - Residual Connection\n","        - [한국어 설명](https://itrepo.tistory.com/36)\n","    - pytorch ModuleList\n","        - [torch official docs](https://pytorch.org/docs/1.9.1/generated/torch.nn.ModuleList.html)\n"]},{"cell_type":"code","execution_count":98,"metadata":{"id":"DjIjUBjN99sc","executionInfo":{"status":"ok","timestamp":1646901661285,"user_tz":-540,"elapsed":13,"user":{"displayName":"김석재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinQOomWONmt82tKAM95-_bF9gvg7EPjUXHZ02Hu4s=s64","userId":"05383809141048124324"}}},"outputs":[],"source":["def clones(module, N):\n","    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)]) #n개 만큼동일한 레이어를 생성\n"]},{"cell_type":"code","source":[""],"metadata":{"id":"1fv0O-E29YjA","executionInfo":{"status":"ok","timestamp":1646901661285,"user_tz":-540,"elapsed":12,"user":{"displayName":"김석재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinQOomWONmt82tKAM95-_bF9gvg7EPjUXHZ02Hu4s=s64","userId":"05383809141048124324"}}},"execution_count":98,"outputs":[]},{"cell_type":"code","execution_count":99,"metadata":{"id":"wgglBAyM99se","executionInfo":{"status":"ok","timestamp":1646901661286,"user_tz":-540,"elapsed":12,"user":{"displayName":"김석재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinQOomWONmt82tKAM95-_bF9gvg7EPjUXHZ02Hu4s=s64","userId":"05383809141048124324"}}},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, layer, N):\n","        super(Encoder, self).__init__()\n","        self.layers = clones(layer, N) #n개의 layer생성\n","        self.norm = LayerNorm(layer.size) #layer normalization , 배치단위가 아닌 인풋된 피처 레이어 단위로 정규화\n","    \n","    \n","    \n","    def forward(self, x, mask):  \n","\n","        for layer in self.layers: #입력값과 마스크를 레이어들을 통과시킨다\n","            x = layer(x, mask)\n","        return self.norm(x)\n"]},{"cell_type":"code","execution_count":100,"metadata":{"id":"AvGP8Gsd99sg","executionInfo":{"status":"ok","timestamp":1646901661287,"user_tz":-540,"elapsed":12,"user":{"displayName":"김석재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinQOomWONmt82tKAM95-_bF9gvg7EPjUXHZ02Hu4s=s64","userId":"05383809141048124324"}}},"outputs":[],"source":["class LayerNorm(nn.Module): #layer normalization\n","    \n","    def __init__(self, features, eps=1e-6):\n","        super(LayerNorm, self).__init__()\n","        self.a_2 = nn.Parameter(torch.ones(features)) #nn.parameter => 레이어가 아닌 파라미터값만 가지고있음  torch.ones => 1로 이루어진 텐서  \n","        self.b_2 = nn.Parameter(torch.zeros(features)) # torch.zeros => 0으로 이루어진 텐서\n","        self.eps = eps\n","   \n","    \n","    \n","    def forward(self, x):\n","        mean = x.mean(-1, keepdim=True) #평균, 구함 입력된 피쳐단위로 , 차원유지 유지\n","        std = x.std(-1, keepdim=True) #표준편차, 위와같음\n","        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2 # ones와 zeros를 왜 쓰는지 모르겠음\n","   "]},{"cell_type":"code","execution_count":101,"metadata":{"id":"525_O3YE99si","executionInfo":{"status":"ok","timestamp":1646901661288,"user_tz":-540,"elapsed":13,"user":{"displayName":"김석재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinQOomWONmt82tKAM95-_bF9gvg7EPjUXHZ02Hu4s=s64","userId":"05383809141048124324"}}},"outputs":[],"source":["class SublayerConnection(nn.Module): #정규화를 해주고 출력값에 입력값을 더해준다, x + 와 sublayer를 하기위해 두값의 차원을 맞춰줘야한다 (이게어느부분이지?)\n","    \n","    def __init__(self, size, dropout):\n","        super(SublayerConnection, self).__init__()\n","        self.norm = LayerNorm(size) #norm 생성\n","        self.dropout = nn.Dropout(dropout) # 드랍아웃은 왜들어가지?\n","   \n","    \n","    def forward(self, x, sublayer):\n","        return x + self.dropout(sublayer(self.norm(x))) #입력값 x와 sublayer를 연결\n","  "]},{"cell_type":"code","execution_count":102,"metadata":{"id":"LlGCPEVp99sk","executionInfo":{"status":"ok","timestamp":1646901662901,"user_tz":-540,"elapsed":1,"user":{"displayName":"김석재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinQOomWONmt82tKAM95-_bF9gvg7EPjUXHZ02Hu4s=s64","userId":"05383809141048124324"}}},"outputs":[],"source":["class EncoderLayer(nn.Module):\n","    \n","    def __init__(self, size, self_attn, feed_forward, dropout):\n","        super(EncoderLayer, self).__init__()\n","        self.self_attn = self_attn\n","        self.feed_forward = feed_forward\n","        self.sublayer = clones(SublayerConnection(size, dropout), 2) #서브레이어를 두개만듬\n","        self.size = size\n","\n","        \n","    def forward(self, x, mask):\n","        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) #쿼리 키 밸류, 마스크 #forward는 선언안하고 써도되는건가??? =>forward() 함수는 model 객체를 데이터와 함께 호출하면 자동으로 실행이됩니다.\n","        return self.sublayer[1](x, self.feed_forward)#인풋 x -> 셀프어텐션 -> x+ ountput x -> 피드포워드 -> x + output x"]},{"cell_type":"markdown","metadata":{"id":"OOiYmYWc99sm"},"source":["### Decoder\n","- `Decoder`\n","- `DecoderLayer`"]},{"cell_type":"code","execution_count":80,"metadata":{"id":"Ik47frFO99so","executionInfo":{"status":"ok","timestamp":1646897265567,"user_tz":-540,"elapsed":429,"user":{"displayName":"김석재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinQOomWONmt82tKAM95-_bF9gvg7EPjUXHZ02Hu4s=s64","userId":"05383809141048124324"}}},"outputs":[],"source":["class Decoder(nn.Module): #인코더와 동일 레이어숫자, 레이어 정규화\n","    \n","    def __init__(self, layer, N):\n","        super(Decoder, self).__init__() \n","        self.layers = clones(layer, N) \n","        self.norm = LayerNorm(layer.size) \n"," \n","    \n","    def forward(self, x, memory, src_mask, tgt_mask):\n","        for layer in self.layers:#입력값과 마스크를 레이어들을 통과시킨다\n","            x = layer(x, memory, src_mask, tgt_mask)\n","        return self.norm(x)"]},{"cell_type":"code","execution_count":81,"metadata":{"id":"ElsG9P7M99sq","executionInfo":{"status":"ok","timestamp":1646897265568,"user_tz":-540,"elapsed":7,"user":{"displayName":"김석재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinQOomWONmt82tKAM95-_bF9gvg7EPjUXHZ02Hu4s=s64","userId":"05383809141048124324"}}},"outputs":[],"source":["class DecoderLayer(nn.Module):\n","\n","    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n","        super(DecoderLayer, self).__init__()\n","        self.size = size\n","        self.self_attn = self_attn\n","        self.src_attn = src_attn\n","        self.feed_forward = feed_forward\n","        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n","     \n","    \n","    \n","    def forward(self, x, memory, src_mask, tgt_mask): #입력값을 출력값에 더해주고 2번째에선 인코더의 출력값과 마스크도 넣어준다 \n","        m = memory\n","        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) #쿼리 키 밸류, 마스크\n","        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask)) #쿼리(이전 디코더 값의 출력) memory 키 memory 밸류, src의 마스크\n","        return self.sublayer[2](x, self.feed_forward) # \n","  "]},{"cell_type":"markdown","metadata":{"id":"vhPP8LVw99sr"},"source":["### Sublayer\n","- `attention` 함수\n","\n","<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_attention.png?raw=true\" width=\"500\" align=\"center\"/>  \n","\n","- `MultiHeadedAttention`\n","- `PositionwiseFeedForward`"]},{"cell_type":"markdown","metadata":{"id":"7o1-iOBu99ss"},"source":["### Challenge\n"]},{"cell_type":"markdown","metadata":{"id":"V0ochH0n99st"},"source":["### Q1. 위 도식에 따라 `score`, `p_attn`, `attention` 을 구하라 "]},{"cell_type":"code","execution_count":110,"metadata":{"id":"ZMYcy8h499sv","executionInfo":{"status":"ok","timestamp":1646905482298,"user_tz":-540,"elapsed":433,"user":{"displayName":"김석재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinQOomWONmt82tKAM95-_bF9gvg7EPjUXHZ02Hu4s=s64","userId":"05383809141048124324"}}},"outputs":[],"source":["def attention(query, key, value, mask=None, dropout=None): #쿼리 키 밸류\n","#Query :  디코더의 이전 레이어 hidden state , 영향을 받는 디코더의 토큰, K : 인코더의 output state , 영향을 주는 인코더의 토큰들 ,V : 인코더의 output state , 그 영향에 대한 가중치가 곱해질 인코더 토큰들\n","#self attention은 q=k=v\n","    d_k = query.size(-1)\n","    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n","             / math.sqrt(d_k)\n","    if mask is not None:\n","        scores = scores.masked_fill(mask == 0, -1e9)\n","    p_attn = F.softmax(scores, dim = -1)\n","    if dropout is not None:\n","        p_attn = dropout(p_attn)        \n","    return torch.matmul(p_attn, value), p_attn #attention , p_attn\n","  "]},{"cell_type":"markdown","metadata":{"id":"x25aeigL99sw"},"source":["###Q2. query, key, value가 모두 (m, d_k) shape의 matrix라고 가정할 때, `score`, `p_attn`, `attention`의 shape을 각각 구하라\n","- score : m,m\n","- p_attn : m,m \n","- attention : m,d_k"]},{"cell_type":"markdown","metadata":{"id":"IHfxLJKz99sx"},"source":["### (아래의 **Q3을 먼저 풀고 돌아오세요**) Q4.  query, key, value가 모두 (12, 8, 1, 64) shape의 tensor라고 가정할 때 , `score`, `p_attn`, `attention`의 shape을 각각 구하라\n","\n","- score : 12, 8, 1, 1\n","- p_attn : 12, 8, 1, 1\n","- attention : 12, 8, 1, 64"]},{"cell_type":"markdown","metadata":{"id":"wYnffQE799sy"},"source":["- `MultiHeadedAttention`\n","\n","<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_multihead.png?raw=true\" width=\"300\" align=\"center\"/>  "]},{"cell_type":"code","execution_count":83,"metadata":{"id":"uhFKlJ2b99sz","executionInfo":{"status":"ok","timestamp":1646897265569,"user_tz":-540,"elapsed":8,"user":{"displayName":"김석재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinQOomWONmt82tKAM95-_bF9gvg7EPjUXHZ02Hu4s=s64","userId":"05383809141048124324"}}},"outputs":[],"source":["class MultiHeadedAttention(nn.Module):\n","    \"\"\"\n","    한번의 어텐션을 하는 것 보다 어텐션을 병렬로 여러번 사용하는것이 더 효과적이다\n","    d_model의 차원을 num_heads개로 나누어 그 값의 차원을 갖는 q,k,v에 대해 num_heads개의 병렬 어텐션을 수행한다 (어텐션 헤드들의 가중치 행렬값은 전부 다름)\n","    다른 시각으로 어텐션 정보를 수집하게된다 \n","    \"\"\"\n","    def __init__(self, h, d_model, dropout=0.1): #h: 헤드숫자, d_model 트랜스포머의 모든 층의 출력 차원 실제 논문에선 512\n","        super(MultiHeadedAttention, self).__init__()\n","        assert d_model % h == 0        \n","    \n","        self.d_k = d_model // h\n","        self.h = h\n","        self.linears = clones(nn.Linear(d_model, d_model), 4)\n","        self.attn = None\n","        self.dropout = nn.Dropout(p=dropout)\n","     \n","    \n","    \n","    def forward(self, query, key, value, mask=None): #멀티어텐션으로 나눠서 계산하고 다시 합친다는건 알겠는데 모르는 코드가 너무많다\n","        if mask is not None:\n","            # 모든 헤드에 동일한 마스크 적용\n","            mask = mask.unsqueeze(1)\n","        nbatches = query.size(0)\n","                \n","        # 1) Do all the linear projections in batch from d_model => h x d_k\n","        query, key, value = \\\n","            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n","             for l, x in zip(self.linears, (query, key, value))]\n","        \n","        # 2) 어텐션을 적용 , Apply attention on all the projected vectors in batch. ??\n","        x, self.attn = attention(query, key, value, mask=mask, \n","                                 dropout=self.dropout)\n","        \n","        # 3) \"Concat\" using a view and apply a final linear. ????          \n","        x = x.transpose(1, 2).contiguous() \\\n","             .view(nbatches, -1, self.h * self.d_k)# \\ = 다음줄이랑 이어지게 해줌 \n","        jj=nn.Linear(512, 512)\n","\n","        return self.linears[-1](x)\n","  \n","            "]},{"cell_type":"markdown","metadata":{"id":"M46Ensa499s0"},"source":["### Q3.  query, key, value가 모두 (12, 512) shape의 matrix이고, h 값이 8 이라고 가정할 때, 아래 값의 shape을 각각 구하라\n","\n","- `d_k` (d_k = d_model // h) : 64\n","- `nn.Linear(d_model, d_model)(query)` : 12, 512\n","- `nn.Linear(d_model, d_model)(query).view(nbatches, -1, h, d_k)` : 12, 1, 8, 64\n","- `nn.Linear(d_model, d_model)(query).view(nbatches, -1, h, d_k).transpose(1,2)` : 12, 8, 1, 64"]},{"cell_type":"markdown","metadata":{"id":"twZoeFr799s1"},"source":["- `PositionwiseFeedForward`\n","\n","<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_pwff.png?raw=true\" width=\"300\" align=\"center\"/>  "]},{"cell_type":"code","execution_count":84,"metadata":{"id":"nZzpucvQ99s2","executionInfo":{"status":"ok","timestamp":1646897265569,"user_tz":-540,"elapsed":7,"user":{"displayName":"김석재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinQOomWONmt82tKAM95-_bF9gvg7EPjUXHZ02Hu4s=s64","userId":"05383809141048124324"}}},"outputs":[],"source":["class PositionwiseFeedForward(nn.Module): #개별 단어마다 적용되기 때문에 position-wise이다 (코드가 왜 이렇게 하는건지 모르겠다) 논문에선 d_model 512 , d_ff 2048\n","    def __init__(self, d_model, d_ff, dropout=0.1): \n","        super(PositionwiseFeedForward, self).__init__()\n","        self.w_1 = nn.Linear(d_model, d_ff)\n","        self.w_2 = nn.Linear(d_ff, d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        return self.w_2(self.dropout(F.relu(self.w_1(x)))) "]},{"cell_type":"markdown","metadata":{"id":"TqjsUsbu99s3"},"source":["### Input Embedding & Encoding\n","- `Embeddings`\n","    - [pytorch official docs](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)"]},{"cell_type":"code","execution_count":85,"metadata":{"id":"FBVJFurO99s3","executionInfo":{"status":"ok","timestamp":1646897265569,"user_tz":-540,"elapsed":7,"user":{"displayName":"김석재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinQOomWONmt82tKAM95-_bF9gvg7EPjUXHZ02Hu4s=s64","userId":"05383809141048124324"}}},"outputs":[],"source":["class Embeddings(nn.Module):\n","  \n","    def __init__(self, d_model, vocab):\n","        super(Embeddings, self).__init__()\n","        self.lut = nn.Embedding(vocab, d_model)\n","        self.d_model = d_model\n","    \n","    \n","    def forward(self, x):\n","        return self.lut(x) * math.sqrt(self.d_model)\n","    "]},{"cell_type":"markdown","metadata":{"id":"Po31qs_A99s5"},"source":["- `PositionalEncoding`\n","\n","<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_pe.png?raw=true\" width=\"500\" align=\"center\"/>  \n","\n","- `position` 변수 설명\n","    - 모든 position (=최대 토큰 개수)의 값을 갖고 있는 matrix\n","- `div_term` 변수 설명\n","\n","<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_div.png?raw=true\" width=\"500\" align=\"center\"/>  \n","- `Embedding` + `Encoding` 도식화 \n","\n","<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_emb_enc.png?raw=true\" width=\"400\" align=\"center\"/>  \n"]},{"cell_type":"code","execution_count":87,"metadata":{"id":"RP-_an3x99s5","executionInfo":{"status":"ok","timestamp":1646897289526,"user_tz":-540,"elapsed":391,"user":{"displayName":"김석재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinQOomWONmt82tKAM95-_bF9gvg7EPjUXHZ02Hu4s=s64","userId":"05383809141048124324"}}},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    \n","    def __init__(self, d_model, dropout, max_len = 5000): #포지션값을 계산하고 입력값과 더해서 인코더나 디코더의 입력으로 넘겨주게된다\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        \n","        # Compute the positional encodings once in log space.\n","        pe = torch.zeros(max_len, d_model) #계산방식은다시한번 살펴봐야할것같다\n","        position = torch.arange(0, max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) *\n","                             -(math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","      \n","        \n","    def forward(self, x):\n","        x = x + Variable(self.pe[:, :x.size(1)], \n","                         requires_grad=False)\n","        return self.dropout(x)\n","      "]},{"cell_type":"markdown","source":[""],"metadata":{"id":"NjANHl92vtQG"}},{"cell_type":"markdown","metadata":{"id":"kNf13Gkm99s6"},"source":["### Q4.  max_len이 512이고, d_model이 512라고 가정할 때, `position`과 `div_term`의 shape을 구하라\n","\n","- `position` : 512,1\n","- `div_term` : 256\n","- `position * div_term` : 512,256"]},{"cell_type":"markdown","metadata":{"id":"Rri-daP399s7"},"source":["### Advanced"]},{"cell_type":"markdown","metadata":{"id":"N3ZixTN199s8"},"source":["### Finally Build Model\n","- Xavier Initialization\n","    - [한국어 자료](https://huangdi.tistory.com/8)\n","    - [pytorch official docs](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_uniform_)"]},{"cell_type":"code","execution_count":88,"metadata":{"id":"kPdGsCiC99s8","executionInfo":{"status":"ok","timestamp":1646897289936,"user_tz":-540,"elapsed":4,"user":{"displayName":"김석재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinQOomWONmt82tKAM95-_bF9gvg7EPjUXHZ02Hu4s=s64","userId":"05383809141048124324"}}},"outputs":[],"source":["def make_model(src_vocab, tgt_vocab,  #왜 모델생성에서는 class를 안쓸까?\n","               N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n","    \"Helper: Construct a model from hyperparameters.\" #연결된다는걸 알겠지만 하나하나 순서를 따라가면서 다시봐야할것같다\n","    c = copy.deepcopy\n","    attn = MultiHeadedAttention(h, d_model)\n","    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n","    position = PositionalEncoding(d_model, dropout)\n","    model = EncoderDecoder(\n","        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n","        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n","                             c(ff), dropout), N),\n","        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n","        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n","        Generator(d_model, tgt_vocab))\n","    \n","    # This was important from their code. \n","    # Initialize parameters with Glorot / fan_avg.\n","    for p in model.parameters():\n","        if p.dim() > 1:\n","            nn.init.xavier_uniform(p)\n","    return model\n","\n","            "]},{"cell_type":"code","execution_count":89,"metadata":{"id":"eIDN1DSd99s-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646897290371,"user_tz":-540,"elapsed":438,"user":{"displayName":"김석재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinQOomWONmt82tKAM95-_bF9gvg7EPjUXHZ02Hu4s=s64","userId":"05383809141048124324"}},"outputId":"2ad2e7f4-c6cf-406b-8e80-29643865c867"},"outputs":[{"output_type":"stream","name":"stdout","text":["1111 64\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"]}],"source":["model = make_model(10,10)"]},{"cell_type":"markdown","metadata":{"id":"ljRK80Lo99s_"},"source":["### Q5. 위 코드로 만든 모델의 모든 파라미터의 이름과 크기 (shape) 을 출력하라"]},{"cell_type":"code","execution_count":94,"metadata":{"id":"BHubCUOh99tA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646897506711,"user_tz":-540,"elapsed":3,"user":{"displayName":"김석재","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinQOomWONmt82tKAM95-_bF9gvg7EPjUXHZ02Hu4s=s64","userId":"05383809141048124324"}},"outputId":"87406f92-2226-46ad-d429-5fed601b29a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([512, 512])\n","encoder.layers.0.self_attn.linears.0.weight\n","torch.Size([512])\n","encoder.layers.0.self_attn.linears.0.bias\n","torch.Size([512, 512])\n","encoder.layers.0.self_attn.linears.1.weight\n","torch.Size([512])\n","encoder.layers.0.self_attn.linears.1.bias\n","torch.Size([512, 512])\n","encoder.layers.0.self_attn.linears.2.weight\n","torch.Size([512])\n","encoder.layers.0.self_attn.linears.2.bias\n","torch.Size([512, 512])\n","encoder.layers.0.self_attn.linears.3.weight\n","torch.Size([512])\n","encoder.layers.0.self_attn.linears.3.bias\n","torch.Size([2048, 512])\n","encoder.layers.0.feed_forward.w_1.weight\n","torch.Size([2048])\n","encoder.layers.0.feed_forward.w_1.bias\n","torch.Size([512, 2048])\n","encoder.layers.0.feed_forward.w_2.weight\n","torch.Size([512])\n","encoder.layers.0.feed_forward.w_2.bias\n","torch.Size([512])\n","encoder.layers.0.sublayer.0.norm.a_2\n","torch.Size([512])\n","encoder.layers.0.sublayer.0.norm.b_2\n","torch.Size([512])\n","encoder.layers.0.sublayer.1.norm.a_2\n","torch.Size([512])\n","encoder.layers.0.sublayer.1.norm.b_2\n","torch.Size([512, 512])\n","encoder.layers.1.self_attn.linears.0.weight\n","torch.Size([512])\n","encoder.layers.1.self_attn.linears.0.bias\n","torch.Size([512, 512])\n","encoder.layers.1.self_attn.linears.1.weight\n","torch.Size([512])\n","encoder.layers.1.self_attn.linears.1.bias\n","torch.Size([512, 512])\n","encoder.layers.1.self_attn.linears.2.weight\n","torch.Size([512])\n","encoder.layers.1.self_attn.linears.2.bias\n","torch.Size([512, 512])\n","encoder.layers.1.self_attn.linears.3.weight\n","torch.Size([512])\n","encoder.layers.1.self_attn.linears.3.bias\n","torch.Size([2048, 512])\n","encoder.layers.1.feed_forward.w_1.weight\n","torch.Size([2048])\n","encoder.layers.1.feed_forward.w_1.bias\n","torch.Size([512, 2048])\n","encoder.layers.1.feed_forward.w_2.weight\n","torch.Size([512])\n","encoder.layers.1.feed_forward.w_2.bias\n","torch.Size([512])\n","encoder.layers.1.sublayer.0.norm.a_2\n","torch.Size([512])\n","encoder.layers.1.sublayer.0.norm.b_2\n","torch.Size([512])\n","encoder.layers.1.sublayer.1.norm.a_2\n","torch.Size([512])\n","encoder.layers.1.sublayer.1.norm.b_2\n","torch.Size([512, 512])\n","encoder.layers.2.self_attn.linears.0.weight\n","torch.Size([512])\n","encoder.layers.2.self_attn.linears.0.bias\n","torch.Size([512, 512])\n","encoder.layers.2.self_attn.linears.1.weight\n","torch.Size([512])\n","encoder.layers.2.self_attn.linears.1.bias\n","torch.Size([512, 512])\n","encoder.layers.2.self_attn.linears.2.weight\n","torch.Size([512])\n","encoder.layers.2.self_attn.linears.2.bias\n","torch.Size([512, 512])\n","encoder.layers.2.self_attn.linears.3.weight\n","torch.Size([512])\n","encoder.layers.2.self_attn.linears.3.bias\n","torch.Size([2048, 512])\n","encoder.layers.2.feed_forward.w_1.weight\n","torch.Size([2048])\n","encoder.layers.2.feed_forward.w_1.bias\n","torch.Size([512, 2048])\n","encoder.layers.2.feed_forward.w_2.weight\n","torch.Size([512])\n","encoder.layers.2.feed_forward.w_2.bias\n","torch.Size([512])\n","encoder.layers.2.sublayer.0.norm.a_2\n","torch.Size([512])\n","encoder.layers.2.sublayer.0.norm.b_2\n","torch.Size([512])\n","encoder.layers.2.sublayer.1.norm.a_2\n","torch.Size([512])\n","encoder.layers.2.sublayer.1.norm.b_2\n","torch.Size([512, 512])\n","encoder.layers.3.self_attn.linears.0.weight\n","torch.Size([512])\n","encoder.layers.3.self_attn.linears.0.bias\n","torch.Size([512, 512])\n","encoder.layers.3.self_attn.linears.1.weight\n","torch.Size([512])\n","encoder.layers.3.self_attn.linears.1.bias\n","torch.Size([512, 512])\n","encoder.layers.3.self_attn.linears.2.weight\n","torch.Size([512])\n","encoder.layers.3.self_attn.linears.2.bias\n","torch.Size([512, 512])\n","encoder.layers.3.self_attn.linears.3.weight\n","torch.Size([512])\n","encoder.layers.3.self_attn.linears.3.bias\n","torch.Size([2048, 512])\n","encoder.layers.3.feed_forward.w_1.weight\n","torch.Size([2048])\n","encoder.layers.3.feed_forward.w_1.bias\n","torch.Size([512, 2048])\n","encoder.layers.3.feed_forward.w_2.weight\n","torch.Size([512])\n","encoder.layers.3.feed_forward.w_2.bias\n","torch.Size([512])\n","encoder.layers.3.sublayer.0.norm.a_2\n","torch.Size([512])\n","encoder.layers.3.sublayer.0.norm.b_2\n","torch.Size([512])\n","encoder.layers.3.sublayer.1.norm.a_2\n","torch.Size([512])\n","encoder.layers.3.sublayer.1.norm.b_2\n","torch.Size([512, 512])\n","encoder.layers.4.self_attn.linears.0.weight\n","torch.Size([512])\n","encoder.layers.4.self_attn.linears.0.bias\n","torch.Size([512, 512])\n","encoder.layers.4.self_attn.linears.1.weight\n","torch.Size([512])\n","encoder.layers.4.self_attn.linears.1.bias\n","torch.Size([512, 512])\n","encoder.layers.4.self_attn.linears.2.weight\n","torch.Size([512])\n","encoder.layers.4.self_attn.linears.2.bias\n","torch.Size([512, 512])\n","encoder.layers.4.self_attn.linears.3.weight\n","torch.Size([512])\n","encoder.layers.4.self_attn.linears.3.bias\n","torch.Size([2048, 512])\n","encoder.layers.4.feed_forward.w_1.weight\n","torch.Size([2048])\n","encoder.layers.4.feed_forward.w_1.bias\n","torch.Size([512, 2048])\n","encoder.layers.4.feed_forward.w_2.weight\n","torch.Size([512])\n","encoder.layers.4.feed_forward.w_2.bias\n","torch.Size([512])\n","encoder.layers.4.sublayer.0.norm.a_2\n","torch.Size([512])\n","encoder.layers.4.sublayer.0.norm.b_2\n","torch.Size([512])\n","encoder.layers.4.sublayer.1.norm.a_2\n","torch.Size([512])\n","encoder.layers.4.sublayer.1.norm.b_2\n","torch.Size([512, 512])\n","encoder.layers.5.self_attn.linears.0.weight\n","torch.Size([512])\n","encoder.layers.5.self_attn.linears.0.bias\n","torch.Size([512, 512])\n","encoder.layers.5.self_attn.linears.1.weight\n","torch.Size([512])\n","encoder.layers.5.self_attn.linears.1.bias\n","torch.Size([512, 512])\n","encoder.layers.5.self_attn.linears.2.weight\n","torch.Size([512])\n","encoder.layers.5.self_attn.linears.2.bias\n","torch.Size([512, 512])\n","encoder.layers.5.self_attn.linears.3.weight\n","torch.Size([512])\n","encoder.layers.5.self_attn.linears.3.bias\n","torch.Size([2048, 512])\n","encoder.layers.5.feed_forward.w_1.weight\n","torch.Size([2048])\n","encoder.layers.5.feed_forward.w_1.bias\n","torch.Size([512, 2048])\n","encoder.layers.5.feed_forward.w_2.weight\n","torch.Size([512])\n","encoder.layers.5.feed_forward.w_2.bias\n","torch.Size([512])\n","encoder.layers.5.sublayer.0.norm.a_2\n","torch.Size([512])\n","encoder.layers.5.sublayer.0.norm.b_2\n","torch.Size([512])\n","encoder.layers.5.sublayer.1.norm.a_2\n","torch.Size([512])\n","encoder.layers.5.sublayer.1.norm.b_2\n","torch.Size([512])\n","encoder.norm.a_2\n","torch.Size([512])\n","encoder.norm.b_2\n","torch.Size([512, 512])\n","decoder.layers.0.self_attn.linears.0.weight\n","torch.Size([512])\n","decoder.layers.0.self_attn.linears.0.bias\n","torch.Size([512, 512])\n","decoder.layers.0.self_attn.linears.1.weight\n","torch.Size([512])\n","decoder.layers.0.self_attn.linears.1.bias\n","torch.Size([512, 512])\n","decoder.layers.0.self_attn.linears.2.weight\n","torch.Size([512])\n","decoder.layers.0.self_attn.linears.2.bias\n","torch.Size([512, 512])\n","decoder.layers.0.self_attn.linears.3.weight\n","torch.Size([512])\n","decoder.layers.0.self_attn.linears.3.bias\n","torch.Size([512, 512])\n","decoder.layers.0.src_attn.linears.0.weight\n","torch.Size([512])\n","decoder.layers.0.src_attn.linears.0.bias\n","torch.Size([512, 512])\n","decoder.layers.0.src_attn.linears.1.weight\n","torch.Size([512])\n","decoder.layers.0.src_attn.linears.1.bias\n","torch.Size([512, 512])\n","decoder.layers.0.src_attn.linears.2.weight\n","torch.Size([512])\n","decoder.layers.0.src_attn.linears.2.bias\n","torch.Size([512, 512])\n","decoder.layers.0.src_attn.linears.3.weight\n","torch.Size([512])\n","decoder.layers.0.src_attn.linears.3.bias\n","torch.Size([2048, 512])\n","decoder.layers.0.feed_forward.w_1.weight\n","torch.Size([2048])\n","decoder.layers.0.feed_forward.w_1.bias\n","torch.Size([512, 2048])\n","decoder.layers.0.feed_forward.w_2.weight\n","torch.Size([512])\n","decoder.layers.0.feed_forward.w_2.bias\n","torch.Size([512])\n","decoder.layers.0.sublayer.0.norm.a_2\n","torch.Size([512])\n","decoder.layers.0.sublayer.0.norm.b_2\n","torch.Size([512])\n","decoder.layers.0.sublayer.1.norm.a_2\n","torch.Size([512])\n","decoder.layers.0.sublayer.1.norm.b_2\n","torch.Size([512])\n","decoder.layers.0.sublayer.2.norm.a_2\n","torch.Size([512])\n","decoder.layers.0.sublayer.2.norm.b_2\n","torch.Size([512, 512])\n","decoder.layers.1.self_attn.linears.0.weight\n","torch.Size([512])\n","decoder.layers.1.self_attn.linears.0.bias\n","torch.Size([512, 512])\n","decoder.layers.1.self_attn.linears.1.weight\n","torch.Size([512])\n","decoder.layers.1.self_attn.linears.1.bias\n","torch.Size([512, 512])\n","decoder.layers.1.self_attn.linears.2.weight\n","torch.Size([512])\n","decoder.layers.1.self_attn.linears.2.bias\n","torch.Size([512, 512])\n","decoder.layers.1.self_attn.linears.3.weight\n","torch.Size([512])\n","decoder.layers.1.self_attn.linears.3.bias\n","torch.Size([512, 512])\n","decoder.layers.1.src_attn.linears.0.weight\n","torch.Size([512])\n","decoder.layers.1.src_attn.linears.0.bias\n","torch.Size([512, 512])\n","decoder.layers.1.src_attn.linears.1.weight\n","torch.Size([512])\n","decoder.layers.1.src_attn.linears.1.bias\n","torch.Size([512, 512])\n","decoder.layers.1.src_attn.linears.2.weight\n","torch.Size([512])\n","decoder.layers.1.src_attn.linears.2.bias\n","torch.Size([512, 512])\n","decoder.layers.1.src_attn.linears.3.weight\n","torch.Size([512])\n","decoder.layers.1.src_attn.linears.3.bias\n","torch.Size([2048, 512])\n","decoder.layers.1.feed_forward.w_1.weight\n","torch.Size([2048])\n","decoder.layers.1.feed_forward.w_1.bias\n","torch.Size([512, 2048])\n","decoder.layers.1.feed_forward.w_2.weight\n","torch.Size([512])\n","decoder.layers.1.feed_forward.w_2.bias\n","torch.Size([512])\n","decoder.layers.1.sublayer.0.norm.a_2\n","torch.Size([512])\n","decoder.layers.1.sublayer.0.norm.b_2\n","torch.Size([512])\n","decoder.layers.1.sublayer.1.norm.a_2\n","torch.Size([512])\n","decoder.layers.1.sublayer.1.norm.b_2\n","torch.Size([512])\n","decoder.layers.1.sublayer.2.norm.a_2\n","torch.Size([512])\n","decoder.layers.1.sublayer.2.norm.b_2\n","torch.Size([512, 512])\n","decoder.layers.2.self_attn.linears.0.weight\n","torch.Size([512])\n","decoder.layers.2.self_attn.linears.0.bias\n","torch.Size([512, 512])\n","decoder.layers.2.self_attn.linears.1.weight\n","torch.Size([512])\n","decoder.layers.2.self_attn.linears.1.bias\n","torch.Size([512, 512])\n","decoder.layers.2.self_attn.linears.2.weight\n","torch.Size([512])\n","decoder.layers.2.self_attn.linears.2.bias\n","torch.Size([512, 512])\n","decoder.layers.2.self_attn.linears.3.weight\n","torch.Size([512])\n","decoder.layers.2.self_attn.linears.3.bias\n","torch.Size([512, 512])\n","decoder.layers.2.src_attn.linears.0.weight\n","torch.Size([512])\n","decoder.layers.2.src_attn.linears.0.bias\n","torch.Size([512, 512])\n","decoder.layers.2.src_attn.linears.1.weight\n","torch.Size([512])\n","decoder.layers.2.src_attn.linears.1.bias\n","torch.Size([512, 512])\n","decoder.layers.2.src_attn.linears.2.weight\n","torch.Size([512])\n","decoder.layers.2.src_attn.linears.2.bias\n","torch.Size([512, 512])\n","decoder.layers.2.src_attn.linears.3.weight\n","torch.Size([512])\n","decoder.layers.2.src_attn.linears.3.bias\n","torch.Size([2048, 512])\n","decoder.layers.2.feed_forward.w_1.weight\n","torch.Size([2048])\n","decoder.layers.2.feed_forward.w_1.bias\n","torch.Size([512, 2048])\n","decoder.layers.2.feed_forward.w_2.weight\n","torch.Size([512])\n","decoder.layers.2.feed_forward.w_2.bias\n","torch.Size([512])\n","decoder.layers.2.sublayer.0.norm.a_2\n","torch.Size([512])\n","decoder.layers.2.sublayer.0.norm.b_2\n","torch.Size([512])\n","decoder.layers.2.sublayer.1.norm.a_2\n","torch.Size([512])\n","decoder.layers.2.sublayer.1.norm.b_2\n","torch.Size([512])\n","decoder.layers.2.sublayer.2.norm.a_2\n","torch.Size([512])\n","decoder.layers.2.sublayer.2.norm.b_2\n","torch.Size([512, 512])\n","decoder.layers.3.self_attn.linears.0.weight\n","torch.Size([512])\n","decoder.layers.3.self_attn.linears.0.bias\n","torch.Size([512, 512])\n","decoder.layers.3.self_attn.linears.1.weight\n","torch.Size([512])\n","decoder.layers.3.self_attn.linears.1.bias\n","torch.Size([512, 512])\n","decoder.layers.3.self_attn.linears.2.weight\n","torch.Size([512])\n","decoder.layers.3.self_attn.linears.2.bias\n","torch.Size([512, 512])\n","decoder.layers.3.self_attn.linears.3.weight\n","torch.Size([512])\n","decoder.layers.3.self_attn.linears.3.bias\n","torch.Size([512, 512])\n","decoder.layers.3.src_attn.linears.0.weight\n","torch.Size([512])\n","decoder.layers.3.src_attn.linears.0.bias\n","torch.Size([512, 512])\n","decoder.layers.3.src_attn.linears.1.weight\n","torch.Size([512])\n","decoder.layers.3.src_attn.linears.1.bias\n","torch.Size([512, 512])\n","decoder.layers.3.src_attn.linears.2.weight\n","torch.Size([512])\n","decoder.layers.3.src_attn.linears.2.bias\n","torch.Size([512, 512])\n","decoder.layers.3.src_attn.linears.3.weight\n","torch.Size([512])\n","decoder.layers.3.src_attn.linears.3.bias\n","torch.Size([2048, 512])\n","decoder.layers.3.feed_forward.w_1.weight\n","torch.Size([2048])\n","decoder.layers.3.feed_forward.w_1.bias\n","torch.Size([512, 2048])\n","decoder.layers.3.feed_forward.w_2.weight\n","torch.Size([512])\n","decoder.layers.3.feed_forward.w_2.bias\n","torch.Size([512])\n","decoder.layers.3.sublayer.0.norm.a_2\n","torch.Size([512])\n","decoder.layers.3.sublayer.0.norm.b_2\n","torch.Size([512])\n","decoder.layers.3.sublayer.1.norm.a_2\n","torch.Size([512])\n","decoder.layers.3.sublayer.1.norm.b_2\n","torch.Size([512])\n","decoder.layers.3.sublayer.2.norm.a_2\n","torch.Size([512])\n","decoder.layers.3.sublayer.2.norm.b_2\n","torch.Size([512, 512])\n","decoder.layers.4.self_attn.linears.0.weight\n","torch.Size([512])\n","decoder.layers.4.self_attn.linears.0.bias\n","torch.Size([512, 512])\n","decoder.layers.4.self_attn.linears.1.weight\n","torch.Size([512])\n","decoder.layers.4.self_attn.linears.1.bias\n","torch.Size([512, 512])\n","decoder.layers.4.self_attn.linears.2.weight\n","torch.Size([512])\n","decoder.layers.4.self_attn.linears.2.bias\n","torch.Size([512, 512])\n","decoder.layers.4.self_attn.linears.3.weight\n","torch.Size([512])\n","decoder.layers.4.self_attn.linears.3.bias\n","torch.Size([512, 512])\n","decoder.layers.4.src_attn.linears.0.weight\n","torch.Size([512])\n","decoder.layers.4.src_attn.linears.0.bias\n","torch.Size([512, 512])\n","decoder.layers.4.src_attn.linears.1.weight\n","torch.Size([512])\n","decoder.layers.4.src_attn.linears.1.bias\n","torch.Size([512, 512])\n","decoder.layers.4.src_attn.linears.2.weight\n","torch.Size([512])\n","decoder.layers.4.src_attn.linears.2.bias\n","torch.Size([512, 512])\n","decoder.layers.4.src_attn.linears.3.weight\n","torch.Size([512])\n","decoder.layers.4.src_attn.linears.3.bias\n","torch.Size([2048, 512])\n","decoder.layers.4.feed_forward.w_1.weight\n","torch.Size([2048])\n","decoder.layers.4.feed_forward.w_1.bias\n","torch.Size([512, 2048])\n","decoder.layers.4.feed_forward.w_2.weight\n","torch.Size([512])\n","decoder.layers.4.feed_forward.w_2.bias\n","torch.Size([512])\n","decoder.layers.4.sublayer.0.norm.a_2\n","torch.Size([512])\n","decoder.layers.4.sublayer.0.norm.b_2\n","torch.Size([512])\n","decoder.layers.4.sublayer.1.norm.a_2\n","torch.Size([512])\n","decoder.layers.4.sublayer.1.norm.b_2\n","torch.Size([512])\n","decoder.layers.4.sublayer.2.norm.a_2\n","torch.Size([512])\n","decoder.layers.4.sublayer.2.norm.b_2\n","torch.Size([512, 512])\n","decoder.layers.5.self_attn.linears.0.weight\n","torch.Size([512])\n","decoder.layers.5.self_attn.linears.0.bias\n","torch.Size([512, 512])\n","decoder.layers.5.self_attn.linears.1.weight\n","torch.Size([512])\n","decoder.layers.5.self_attn.linears.1.bias\n","torch.Size([512, 512])\n","decoder.layers.5.self_attn.linears.2.weight\n","torch.Size([512])\n","decoder.layers.5.self_attn.linears.2.bias\n","torch.Size([512, 512])\n","decoder.layers.5.self_attn.linears.3.weight\n","torch.Size([512])\n","decoder.layers.5.self_attn.linears.3.bias\n","torch.Size([512, 512])\n","decoder.layers.5.src_attn.linears.0.weight\n","torch.Size([512])\n","decoder.layers.5.src_attn.linears.0.bias\n","torch.Size([512, 512])\n","decoder.layers.5.src_attn.linears.1.weight\n","torch.Size([512])\n","decoder.layers.5.src_attn.linears.1.bias\n","torch.Size([512, 512])\n","decoder.layers.5.src_attn.linears.2.weight\n","torch.Size([512])\n","decoder.layers.5.src_attn.linears.2.bias\n","torch.Size([512, 512])\n","decoder.layers.5.src_attn.linears.3.weight\n","torch.Size([512])\n","decoder.layers.5.src_attn.linears.3.bias\n","torch.Size([2048, 512])\n","decoder.layers.5.feed_forward.w_1.weight\n","torch.Size([2048])\n","decoder.layers.5.feed_forward.w_1.bias\n","torch.Size([512, 2048])\n","decoder.layers.5.feed_forward.w_2.weight\n","torch.Size([512])\n","decoder.layers.5.feed_forward.w_2.bias\n","torch.Size([512])\n","decoder.layers.5.sublayer.0.norm.a_2\n","torch.Size([512])\n","decoder.layers.5.sublayer.0.norm.b_2\n","torch.Size([512])\n","decoder.layers.5.sublayer.1.norm.a_2\n","torch.Size([512])\n","decoder.layers.5.sublayer.1.norm.b_2\n","torch.Size([512])\n","decoder.layers.5.sublayer.2.norm.a_2\n","torch.Size([512])\n","decoder.layers.5.sublayer.2.norm.b_2\n","torch.Size([512])\n","decoder.norm.a_2\n","torch.Size([512])\n","decoder.norm.b_2\n","torch.Size([10, 512])\n","src_embed.0.lut.weight\n","torch.Size([10, 512])\n","tgt_embed.0.lut.weight\n","torch.Size([10, 512])\n","generator.proj.weight\n","torch.Size([10])\n","generator.proj.bias\n"]}],"source":["for name, param in model.named_parameters():\n","    print(param.size())\n","    print(name)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"7팀_김석재 3-4.ipynb","provenance":[{"file_id":"1FO1d5-UYxY5ldxx61bdRmwPjPX4bT5Au","timestamp":1646891846976}]},"kernelspec":{"display_name":"torch","language":"python","name":"torch"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"}},"nbformat":4,"nbformat_minor":0}